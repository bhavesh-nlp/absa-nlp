{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbeaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05_bert_bio_pol_training\n",
    "\n",
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "from src.config import PROCESSED_DIR, ASPECT_MODEL_DIR\n",
    "\n",
    "\n",
    "# Load BIO-POL datasets\n",
    "train_df = pd.read_parquet(PROCESSED_DIR / \"bio_pol_train.parquet\")\n",
    "val_df   = pd.read_parquet(PROCESSED_DIR / \"bio_pol_val.parquet\")\n",
    "test_df  = pd.read_parquet(PROCESSED_DIR / \"bio_pol_test.parquet\")\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "\n",
    "# Define label\n",
    "label_list = [\n",
    "    \"O\",\n",
    "    \"B-POS\", \"I-POS\",\n",
    "    \"B-NEG\", \"I-NEG\",\n",
    "    \"B-NEU\", \"I-NEU\"\n",
    "]\n",
    "\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in id2label.items()}\n",
    "\n",
    "\n",
    "# load tokenizer & model\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Build HuggingFace Dataset wrapper\n",
    "class ABSA_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df[\"input_ids\"].tolist()\n",
    "        self.labels = df[\"labels\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_dataset = ABSA_Dataset(train_df)\n",
    "val_dataset   = ABSA_Dataset(val_df)\n",
    "test_dataset  = ABSA_Dataset(test_df)\n",
    "\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# f1-score\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(ASPECT_MODEL_DIR),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# model training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluation\n",
    "test_results = trainer.predict(test_dataset)\n",
    "\n",
    "preds = np.argmax(test_results.predictions, axis=2)\n",
    "labels = test_results.label_ids\n",
    "\n",
    "true_preds = [\n",
    "    [id2label[p] for (p, l) in zip(pred, lab) if l != -100]\n",
    "    for pred, lab in zip(preds, labels)\n",
    "]\n",
    "\n",
    "true_labels = [\n",
    "    [id2label[l] for (p, l) in zip(pred, lab) if l != -100]\n",
    "    for pred, lab in zip(preds, labels)\n",
    "]\n",
    "\n",
    "print(classification_report(true_labels, true_preds))\n",
    "\n",
    "\n",
    "# Save Model\n",
    "trainer.save_model(ASPECT_MODEL_DIR)\n",
    "tokenizer.save_pretrained(ASPECT_MODEL_DIR)\n",
    "\n",
    "print(\"Model saved to:\", ASPECT_MODEL_DIR)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
